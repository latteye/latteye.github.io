<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 通过 bonding 以及 irq 分配达到网卡负载平衡 · Water is</title><meta name="description" content="通过 bonding 以及 irq 分配达到网卡负载平衡 - latteye"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://latteye.me/atom.xml" title="Water is"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://www.zhihu.com/people/yuan-hao-yang" target="_blank" class="nav-list-link">ZHIHU</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">通过 bonding 以及 irq 分配达到网卡负载平衡</h1><div class="post-info">2009年2月2日</div><div class="post-content"><p>当网络的并发达到一定数量，由于网卡软中断量很大，引起 CPU 负载升高。<br>在解决此问题时，需要同时考虑网络负载以及 CPU 负载。于是就形成了 bonding 以及 irq 分配共用的思路。<br>这种需求也只有在网络引起机器负载较高的情况下才需要去做。<br><a id="more"></a></p>
<p>本文只是记录一些要点以供备忘。</p>
<p>先看一下我的笔记本上的网卡，没有作 irqbalance</p>
<p><code>cat /proc/interrupts  | sed -n &#39;1,10p&#39;
           CPU0       CPU1       
 17:      14435          0   IO-APIC-fasteoi   eth0</code></p>
<p>现在看一下在服务器上完成配置之后的效果：<br>`<br>           CPU0       CPU1       CPU2       CPU3       CPU4       CPU5       CPU6       CPU7       </p>
<p>106:         41        377      57316     722205        566   17640513      11550         22         PCI-MSI  eth2<br>114:         25     189105      14474     189953         73       4999      91385   18817872         PCI-MSI  eth3<br>169:     146567    5126008      53490   32161155      14819    4368316      71727          0   IO-APIC-level  eth1<br>177:     300116   29877421       4173    4859476      30524          0      10445    7413979   IO-APIC-level  ioc0, eth0`</p>
<p><strong>一、bonding</strong></p>
<p>多网卡 bonding 时，主要考虑 bonding 的模式。可以参考 <a href="http://linuxfoundation.org/en/Net:Bonding" target="_blank" rel="external">这里的文章</a> 来详细查看 bonding 模式。</p>
<p>在这次测试中，双千兆网卡 bonding 所能达到的极限速度为 1.8G，排除 TCP 包 ACK 包，几乎达到了 2G 的速率。<br>四千兆网卡未测速度。</p>
<p>配置就不细说了，外面资料很多。</p>
<p><strong>二、irqbalance</strong></p>
<p><strong>1. broadcom 网卡问题</strong><br>在 irqbalance 的主页上提到过，其功能是基于 intel 芯片的。由于没有测试其他类型的主板，也无法下定论。<br>在测试中，两个 intel 的千兆网卡 irq 分配上没有遇到任何问题。而两块 Broadcom 的网卡在开启 <a href="http://lwn.net/Articles/44139/" target="_blank" rel="external">MSI 功能</a>后（默认即开启 MSI），修改 irq smp_affinity 配置并不生效。只有在关闭 MSI 后才可以正常工作。在 /etc/modprobe.conf 加入 _options bnx2 disable<em>msi=1</em> 即可关闭 MSI。</p>
<p><strong>2.irq 手动切换问题</strong></p>
<p>irqbalance 确实是一个不错的工具。但当我多个网卡负载较高时，他依然有可能将多个网卡的负载分配到一个 CPU 上。所以手动修改 irq smp_affinity 是必要的。</p>
<p>关于 irq 的手动分配，可以参考两篇资料：</p>
<p><a href="http://www.mjmwired.net/kernel/Documentation/ia64/IRQ-redir.txt" target="_blank" rel="external">http://www.mjmwired.net/kernel/Documentation/ia64/IRQ-redir.txt</a><br><a href="http://www.cs.uwaterloo.ca/~brecht/servers/apic/SMP-affinity.txt" target="_blank" rel="external">http://www.cs.uwaterloo.ca/~brecht/servers/apic/SMP-affinity.txt</a></p>
<p>简单的说来，这么计算一下即可：</p>
<p>比如 eth3 的 irq 是 114 。我希望它的负载都由 CPU5 负责。 2 的 5 次方为 32（十进制），其十六进制为 20 。那么运行 echo “20” /proc/irq/114/smp_affinity 就可以了。</p>
<p>写起来才几个字，做测试做了好几天。劳动皆不易阿。</p>
</div></article></div></main><footer><div class="paginator"><a href="/2009/02/oracle-rac-e5-9c-a8-centos-52-64-e4-bd-8d-e4-b8-ad-e5-ae-89-e8-a3-85-e8-a6-81-e7-82-b9.html" class="prev">PREV</a><a href="/2009/01/x64-e4-b8-8a-e7-b3-bb-e7-bb-9f-e6-97-b6-e9-97-b4-e4-b8-8d-e5-87-86-e7-a1-ae-e7-9a-84-e8-a7-a3-e5-86-b3-e6-96-b9-e5-bc-8f.html" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2017 <a href="http://latteye.me">latteye</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>